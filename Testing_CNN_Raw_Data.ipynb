{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(s):\n",
    "    # retrieves all the filenames in a list of strings\n",
    "    path = './image_data/PetImages/{}'.format(s)\n",
    "    vals = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            if os.path.getsize(path + '/'+ filename) == 0:\n",
    "                continue\n",
    "            vals.append(filename)\n",
    "    return sorted(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_filepath(img_name):\n",
    "    # Returns the filepath of a given string\n",
    "    return './image_data/PetImages/cat/{}'.format(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dog_train_filepath(img_name):\n",
    "    # Returns the filepath of a given string\n",
    "    return './image_data/PetImages/dogs_train/{}'.format(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dog_test_filepath(img_name):\n",
    "    # Returns the filepath of a given string\n",
    "    return './image_data/PetImages/dogs_test/{}'.format(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing to check is to see how the images' pixels average values look\n",
    "def tonp(list_of_images, size=(200, 200)):\n",
    "    # for img in list_of_images:\n",
    "    path = get_cat_filepath(list_of_images)\n",
    "    # Transforming all the images to size 400x400\n",
    "    current_img = image.load_img(path, target_size=size, color_mode='grayscale')\n",
    "    # makes a matrix\n",
    "    img_ts = image.img_to_array(current_img)\n",
    "    # converts to a vector\n",
    "    img_ts = img_ts.ravel()\n",
    "    current_img.close()\n",
    "    try:\n",
    "        # Brings all the new vectors into one giant array\n",
    "        full_mat = np.concatenate((full_mat, img_ts))\n",
    "    except UnboundLocalError:\n",
    "        full_mat = img_ts\n",
    "    return full_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing to check is to see how the images' pixels average values look\n",
    "def tonp_dog_train(list_of_images, size=(200, 200)):\n",
    "    # for img in list_of_images:\n",
    "    path = get_dog_train_filepath(list_of_images)\n",
    "    # Transforming all the images to size 400x400\n",
    "    current_img = image.load_img(path, target_size=size, color_mode='grayscale')\n",
    "    # makes a matrix\n",
    "    img_ts = image.img_to_array(current_img)\n",
    "    # converts to a vector\n",
    "    img_ts = img_ts.ravel()\n",
    "    current_img.close()\n",
    "    try:\n",
    "        # Brings all the new vectors into one giant array\n",
    "        full_mat = np.concatenate((full_mat, img_ts))\n",
    "    except UnboundLocalError:\n",
    "        full_mat = img_ts\n",
    "    return full_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing to check is to see how the images' pixels average values look\n",
    "def tonp_dog_test(list_of_images, size=(200, 200)):\n",
    "    # for img in list_of_images:\n",
    "    path = get_dog_test_filepath(list_of_images)\n",
    "    # Transforming all the images to size 400x400\n",
    "    current_img = image.load_img(path, target_size=size, color_mode='grayscale')\n",
    "    # makes a matrix\n",
    "    img_ts = image.img_to_array(current_img)\n",
    "    # converts to a vector\n",
    "    img_ts = img_ts.ravel()\n",
    "    current_img.close()\n",
    "    try:\n",
    "        # Brings all the new vectors into one giant array\n",
    "        full_mat = np.concatenate((full_mat, img_ts))\n",
    "    except UnboundLocalError:\n",
    "        full_mat = img_ts\n",
    "    return full_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_np(np_array):\n",
    "    # The functiton takes in an np_array to display the image\n",
    "    # This will display the image in grayscale\n",
    "    plt.imshow(np_array, vmin=0, vmax=255, cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    cat_filenames = get_file_names('cat')\n",
    "    cat_filenames = cat_filenames[1:len(cat_filenames)]\n",
    "    cat_filenames = cat_filenames[:len(cat_filenames) - 1]\n",
    "    dog_filenames = get_file_names('dogs_train')\n",
    "    dog_test_filenames = get_file_names('dogs_test')\n",
    "    # Testing CNN prediction on raw data\n",
    "    pool = multiprocessing.Pool()\n",
    "    # Will get the data in a matrix for the cata data\n",
    "    raw_data = pool.map(tonp, cat_filenames)\n",
    "    # Needs to do this for each of the train and test data of the dog images\n",
    "    dog_train_data = pool.map(tonp_dog_train, dog_filenames)\n",
    "    dog_test_data = pool.map(tonp_dog_test, dog_test_filenames)\n",
    "    # Returns the array in array([[]])\n",
    "    dog_train_data = np.asarray(dog_train_data).reshape(len(dog_train_data), 200, 200, 1)\n",
    "    dog_test_data = np.asarray(dog_test_data).reshape(len(dog_test_data), 200, 200, 1)\n",
    "    raw_data = np.asarray(raw_data)\n",
    "    # Splits the data by 70% for the cat data\n",
    "    lower_split = int(np.ceil(len(raw_data) * .7))\n",
    "    X_train = raw_data[:lower_split].reshape(lower_split, 200, 200, 1)\n",
    "    X_test = raw_data[lower_split:].reshape(len(raw_data) - lower_split, 200, 200 ,1)\n",
    "    # Creates the corresponding labels for each image\n",
    "    dog_y_train = np.array([0 for _ in range(len(dog_train_data))])\n",
    "    y_train = np.array([1 for _ in range(len(X_train))])\n",
    "    # Does the same for the testing data\n",
    "    dog_y_test = np.array([0 for _ in range(len(dog_test_data))])\n",
    "    y_test = np.array([1 for _ in range(len(X_test))])\n",
    "    # Joins everything together and has everything split into training and testing data\n",
    "    X_train = np.concatenate((X_train, dog_train_data))\n",
    "    X_test = np.concatenate((X_test, dog_test_data))\n",
    "    y_train = np.concatenate((y_train, dog_y_train))\n",
    "    y_test = np.concatenate((y_test, dog_y_test))\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    return X_train, y_train, X_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_filenames = get_file_names('cat')\n",
    "cat_filenames = cat_filenames[1:len(cat_filenames)]\n",
    "cat_filenames = cat_filenames[:len(cat_filenames) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dog_filenames = get_file_names('dogs_train')\n",
    "dog_test_filenames = get_file_names('dogs_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing CNN prediction on raw data\n",
    "pool = multiprocessing.Pool()\n",
    "raw_data = pool.map(tonp, cat_filenames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dog_train_data = pool.map(tonp_dog_train, dog_filenames)\n",
    "dog_test_data = pool.map(tonp_dog_test, dog_test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_train_data = np.asarray(dog_train_data).reshape(len(dog_train_data), 200, 200, 1)\n",
    "dog_test_data = np.asarray(dog_test_data).reshape(len(dog_test_data), 200, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([166., 168., 172., ...,   0.,   0.,   0.], dtype=float32),\n",
       " array([101.,  98., 100., ...,  37.,  46.,  47.], dtype=float32),\n",
       " array([255., 255., 255., ..., 255., 254., 231.], dtype=float32),\n",
       " array([39., 40., 40., ..., 43., 50., 29.], dtype=float32),\n",
       " array([ 31.,  22.,  13., ..., 169., 156., 149.], dtype=float32),\n",
       " array([223., 223., 223., ..., 210., 213., 213.], dtype=float32),\n",
       " array([111., 112., 112., ...,  78.,  78.,  78.], dtype=float32),\n",
       " array([ 8.,  8.,  8., ..., 11.,  9., 11.], dtype=float32),\n",
       " array([121., 120., 115., ..., 162., 164., 161.], dtype=float32),\n",
       " array([60., 60., 62., ..., 32., 21., 23.], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[166., 168., 172., ...,   0.,   0.,   0.],\n",
       "       [101.,  98., 100., ...,  37.,  46.,  47.],\n",
       "       [255., 255., 255., ..., 255., 254., 231.],\n",
       "       ...,\n",
       "       [  8.,   8.,   8., ...,  11.,   9.,  11.],\n",
       "       [121., 120., 115., ..., 162., 164., 161.],\n",
       "       [ 60.,  60.,  62., ...,  32.,  21.,  23.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.asarray(raw_data)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_split = int(np.ceil(len(raw_data) * .7))\n",
    "X_train = raw_data[:lower_split].reshape(lower_split, 200, 200, 1)\n",
    "X_test = raw_data[lower_split:].reshape(len(raw_data) - lower_split, 200, 200 ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dog_y_train = np.array([0 for _ in range(len(dog_train_data))])\n",
    "y_train = np.array([1 for _ in range(len(X_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2] = 0\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_y_test = np.array([0 for _ in range(len(dog_test_data))])\n",
    "y_test = np.array([1 for _ in range(len(X_test))])\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, dog_train_data))\n",
    "X_test = np.concatenate((X_test, dog_test_data))\n",
    "y_train = np.concatenate((y_train, dog_y_train))\n",
    "y_test = np.concatenate((y_test, dog_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[166.],\n",
       "         [168.],\n",
       "         [172.],\n",
       "         ...,\n",
       "         [206.],\n",
       "         [204.],\n",
       "         [202.]],\n",
       "\n",
       "        [[166.],\n",
       "         [168.],\n",
       "         [172.],\n",
       "         ...,\n",
       "         [206.],\n",
       "         [204.],\n",
       "         [203.]],\n",
       "\n",
       "        [[166.],\n",
       "         [168.],\n",
       "         [172.],\n",
       "         ...,\n",
       "         [208.],\n",
       "         [205.],\n",
       "         [203.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[124.],\n",
       "         [125.],\n",
       "         [126.],\n",
       "         ...,\n",
       "         [  2.],\n",
       "         [  2.],\n",
       "         [  2.]],\n",
       "\n",
       "        [[123.],\n",
       "         [123.],\n",
       "         [124.],\n",
       "         ...,\n",
       "         [  1.],\n",
       "         [  1.],\n",
       "         [  1.]],\n",
       "\n",
       "        [[121.],\n",
       "         [122.],\n",
       "         [123.],\n",
       "         ...,\n",
       "         [  0.],\n",
       "         [  0.],\n",
       "         [  0.]]],\n",
       "\n",
       "\n",
       "       [[[101.],\n",
       "         [ 98.],\n",
       "         [100.],\n",
       "         ...,\n",
       "         [126.],\n",
       "         [126.],\n",
       "         [126.]],\n",
       "\n",
       "        [[ 97.],\n",
       "         [ 97.],\n",
       "         [ 98.],\n",
       "         ...,\n",
       "         [127.],\n",
       "         [126.],\n",
       "         [129.]],\n",
       "\n",
       "        [[ 98.],\n",
       "         [ 97.],\n",
       "         [ 99.],\n",
       "         ...,\n",
       "         [128.],\n",
       "         [129.],\n",
       "         [129.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[108.],\n",
       "         [ 99.],\n",
       "         [104.],\n",
       "         ...,\n",
       "         [ 49.],\n",
       "         [ 50.],\n",
       "         [ 52.]],\n",
       "\n",
       "        [[103.],\n",
       "         [103.],\n",
       "         [109.],\n",
       "         ...,\n",
       "         [ 39.],\n",
       "         [ 46.],\n",
       "         [ 56.]],\n",
       "\n",
       "        [[102.],\n",
       "         [109.],\n",
       "         [ 99.],\n",
       "         ...,\n",
       "         [ 37.],\n",
       "         [ 46.],\n",
       "         [ 47.]]],\n",
       "\n",
       "\n",
       "       [[[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [255.],\n",
       "         [255.]],\n",
       "\n",
       "        [[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [255.],\n",
       "         [255.]],\n",
       "\n",
       "        [[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [255.],\n",
       "         [255.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [254.],\n",
       "         [231.]],\n",
       "\n",
       "        [[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [254.],\n",
       "         [231.]],\n",
       "\n",
       "        [[255.],\n",
       "         [255.],\n",
       "         [255.],\n",
       "         ...,\n",
       "         [255.],\n",
       "         [254.],\n",
       "         [231.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 31.],\n",
       "         [ 22.],\n",
       "         [ 13.],\n",
       "         ...,\n",
       "         [153.],\n",
       "         [150.],\n",
       "         [148.]],\n",
       "\n",
       "        [[ 41.],\n",
       "         [ 44.],\n",
       "         [ 38.],\n",
       "         ...,\n",
       "         [152.],\n",
       "         [151.],\n",
       "         [149.]],\n",
       "\n",
       "        [[ 43.],\n",
       "         [ 44.],\n",
       "         [ 46.],\n",
       "         ...,\n",
       "         [154.],\n",
       "         [151.],\n",
       "         [150.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[163.],\n",
       "         [156.],\n",
       "         [155.],\n",
       "         ...,\n",
       "         [170.],\n",
       "         [157.],\n",
       "         [147.]],\n",
       "\n",
       "        [[150.],\n",
       "         [133.],\n",
       "         [133.],\n",
       "         ...,\n",
       "         [170.],\n",
       "         [156.],\n",
       "         [148.]],\n",
       "\n",
       "        [[140.],\n",
       "         [133.],\n",
       "         [141.],\n",
       "         ...,\n",
       "         [169.],\n",
       "         [156.],\n",
       "         [149.]]],\n",
       "\n",
       "\n",
       "       [[[223.],\n",
       "         [223.],\n",
       "         [223.],\n",
       "         ...,\n",
       "         [233.],\n",
       "         [241.],\n",
       "         [245.]],\n",
       "\n",
       "        [[222.],\n",
       "         [222.],\n",
       "         [223.],\n",
       "         ...,\n",
       "         [238.],\n",
       "         [240.],\n",
       "         [247.]],\n",
       "\n",
       "        [[219.],\n",
       "         [221.],\n",
       "         [222.],\n",
       "         ...,\n",
       "         [238.],\n",
       "         [239.],\n",
       "         [237.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[212.],\n",
       "         [210.],\n",
       "         [210.],\n",
       "         ...,\n",
       "         [211.],\n",
       "         [213.],\n",
       "         [212.]],\n",
       "\n",
       "        [[214.],\n",
       "         [208.],\n",
       "         [210.],\n",
       "         ...,\n",
       "         [210.],\n",
       "         [213.],\n",
       "         [213.]],\n",
       "\n",
       "        [[214.],\n",
       "         [209.],\n",
       "         [210.],\n",
       "         ...,\n",
       "         [210.],\n",
       "         [213.],\n",
       "         [213.]]],\n",
       "\n",
       "\n",
       "       [[[111.],\n",
       "         [112.],\n",
       "         [112.],\n",
       "         ...,\n",
       "         [ 88.],\n",
       "         [ 88.],\n",
       "         [ 60.]],\n",
       "\n",
       "        [[111.],\n",
       "         [112.],\n",
       "         [112.],\n",
       "         ...,\n",
       "         [ 90.],\n",
       "         [ 90.],\n",
       "         [ 58.]],\n",
       "\n",
       "        [[111.],\n",
       "         [112.],\n",
       "         [112.],\n",
       "         ...,\n",
       "         [ 90.],\n",
       "         [ 90.],\n",
       "         [ 58.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 65.],\n",
       "         [ 58.],\n",
       "         [ 58.],\n",
       "         ...,\n",
       "         [ 93.],\n",
       "         [ 93.],\n",
       "         [ 87.]],\n",
       "\n",
       "        [[ 65.],\n",
       "         [ 58.],\n",
       "         [ 58.],\n",
       "         ...,\n",
       "         [ 93.],\n",
       "         [ 93.],\n",
       "         [ 87.]],\n",
       "\n",
       "        [[ 55.],\n",
       "         [ 54.],\n",
       "         [ 54.],\n",
       "         ...,\n",
       "         [ 78.],\n",
       "         [ 78.],\n",
       "         [ 78.]]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12751, 200, 200, 1) (12751,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4750, 200, 200, 1), (4750,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "399/399 [==============================] - 1170s 3s/step - loss: 0.0000e+00 - accuracy: 0.6810 - val_loss: 0.0000e+00 - val_accuracy: 0.7895\n",
      "Epoch 2/3\n",
      "399/399 [==============================] - 3523s 9s/step - loss: 0.0000e+00 - accuracy: 0.6789 - val_loss: 0.0000e+00 - val_accuracy: 0.7895\n",
      "Epoch 3/3\n",
      "399/399 [==============================] - 1140s 3s/step - loss: 0.0000e+00 - accuracy: 0.6884 - val_loss: 0.0000e+00 - val_accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa186491050>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input_shape = (height, width, 1 if it's grayscale)\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(200, 200, 1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
